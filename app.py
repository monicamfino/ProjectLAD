import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import time
from sklearn.linear_model import Ridge, Lasso, LinearRegression
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

# Configura√ß√£o da p√°gina
st.set_page_config(page_title="BugsBunny - Detec√ß√£o de Fraude üí≥", layout="wide")

# Estilizando o app
st.markdown("""
    <style>
        .big-font { font-size:30px !important; font-weight: bold; }
        .stApp { background-color: #f5f5f5; }
    </style>
""", unsafe_allow_html=True)


# Carregar os dados
@st.cache_data
def load_data():
    time.sleep(2)
    df = pd.read_csv("creditcard.csv")
    df = df.dropna()
    df["Hour"] = (df["Time"] // 3600) % 24

    # Criar novas features
    df["Rolling_Mean_Amount"] = df["Amount"].rolling(window=5).mean()
    df["Std_Amount"] = df["Amount"].rolling(window=5).std()
    df["Delta_Amount"] = df["Amount"].diff()
    df["Amount_Category"] = pd.cut(df["Amount"], bins=[0, 10, 50, 100, 500, 5000, np.inf],
                                   labels=["Very Low", "Low", "Medium", "High", "Very High", "Extreme"])
    df["Time_Diff"] = df["Time"].diff()
    df["Transacao_Noturna"] = df["Hour"].apply(lambda x: 1 if x < 6 else 0)
    df["Num_Transacoes_1h"] = df.groupby("Hour")["Time"].transform("count")
    df["Freq_Valor_Transacao"] = df.groupby("Amount")["Amount"].transform("count")
    df["Delta_Media_Valor"] = df["Amount"] - df["Rolling_Mean_Amount"]

    np.random.seed(42)
    df["Region"] = np.random.choice(["Norte", "Sul", "Leste", "Oeste", "Centro"], size=len(df))
    return df


df = load_data()

# Criar Sidebar (Menu lateral)
st.sidebar.image("https://cdn-icons-png.flaticon.com/512/1041/1041916.png", width=100)
st.sidebar.title("üê∞ BugsBunny Analytics")
st.sidebar.write("Solu√ß√µes Inteligentes para a Detec√ß√£o de Fraudes")
page = st.sidebar.radio("Navega√ß√£o", [
    "üè† Vis√£o Geral",
    "üìä An√°lise de Fraudes",
    "üìà Estat√≠sticas",
    "üìÇ Relat√≥rios e Configura√ß√µes",
    "üß≠ Dados",
    "ü§ñ Machine Learning"
])

# üìå P√°gina Inicial - Vis√£o Geral
if page == "üè† Vis√£o Geral":
    st.markdown('<p class="big-font">üîç Vis√£o Geral - Detec√ß√£o de Fraude</p>', unsafe_allow_html=True)

    # üè¢ Sobre a Plataforma
    st.subheader("üíº Sobre o BugsBunny Analytics")
    st.write("""
    A nossa miss√£o √© ajudar empresas a detectarem fraudes financeiras com intelig√™ncia artificial e an√°lise de dados.
    Oferecemos solu√ß√µes para monitoriza√ß√£o, preven√ß√£o e identifica√ß√£o de atividades suspeitas.
    """)

    # üìú Tipos Comuns de Fraude
    st.subheader("üìú Tipos Comuns de Fraude")
    fraud_types = pd.DataFrame({
        "Tipo de Fraude": ["Fraude em Cart√£o de Cr√©dito", "Phishing", "Roubo de Identidade", "Transa√ß√µes Falsificadas"],
        "Descri√ß√£o": [
            "Utiliza√ß√£o n√£o autorizada do cart√£o para compras.",
            "Enganar utilizadores para fornecerem informa√ß√µes sens√≠veis.",
            "Falsifica√ß√£o de identidade para acesso financeiro il√≠cito.",
            "Manipula√ß√£o ou falsifica√ß√£o de transa√ß√µes banc√°rias."
        ]
    })
    st.table(fraud_types)

    # üìä Estat√≠sticas Gerais
    total_transacoes = len(df)
    transacoes_fraudulentas = df["Class"].sum()
    taxa_fraude = (transacoes_fraudulentas / total_transacoes) * 100

    col1, col2, col3 = st.columns(3)
    col1.metric("üí≥ Total de Transa√ß√µes", f"{total_transacoes:,}")
    col2.metric("‚ö† Transa√ß√µes Fraudulentas", f"{transacoes_fraudulentas:,}")
    col3.metric("üìâ Taxa de Fraude", f"{taxa_fraude:.2f} %")

    # üõ†Ô∏è Vari√°veis Utilizadas no Modelo e no CSV
    st.subheader("üõ†Ô∏è Vari√°veis Utilizadas no Modelo e no CSV")
    variaveis_combinadas = pd.DataFrame({
        "Vari√°vel": [
            "Time", "V1-V28", "Amount", "Class",
            "Hour", "Rolling_Mean_Amount", "Std_Amount", "Delta_Amount",
            "Amount_Category", "Time_Diff", "Transacao_Noturna",
            "Num_Transacoes_1h", "Freq_Valor_Transacao", "Delta_Media_Valor", "Region"
        ],
        "Descri√ß√£o": [
            "Tempo decorrido desde a primeira transa√ß√£o no dataset.",
            "Vari√°veis anonimizadas resultantes de PCA (28 componentes principais).",
            "Montante da transa√ß√£o.",
            "Classe da transa√ß√£o (0: Leg√≠tima, 1: Fraudulenta).",
            "Hora do dia em que a transa√ß√£o ocorreu.",
            "M√©dia m√≥vel do valor da transa√ß√£o (janela de 5 transa√ß√µes).",
            "Desvio padr√£o do valor da transa√ß√£o (janela de 5 transa√ß√µes).",
            "Diferen√ßa entre o valor atual e o valor anterior da transa√ß√£o.",
            "Categoria do valor da transa√ß√£o (ex.: Muito Baixo, Baixo, M√©dio, etc.).",
            "Diferen√ßa de tempo entre transa√ß√µes consecutivas.",
            "Indica se a transa√ß√£o ocorreu durante a noite (1: Sim, 0: N√£o).",
            "N√∫mero de transa√ß√µes realizadas na mesma hora.",
            "Frequ√™ncia de transa√ß√µes com o mesmo valor.",
            "Diferen√ßa entre o valor da transa√ß√£o e a m√©dia m√≥vel.",
            "Regi√£o geogr√°fica associada √† transa√ß√£o."
        ]
    })
    st.table(variaveis_combinadas)

    # üõ°Ô∏è Como Prevenir Fraudes?
    st.subheader("üõ°Ô∏è Como Prevenir Fraudes?")
    st.write("""
    A preven√ß√£o de fraudes envolve um conjunto de boas pr√°ticas e tecnologias que ajudam a proteger empresas e consumidores. 
    Aqui est√£o algumas recomenda√ß√µes essenciais:
    """)

    fraud_prevention = pd.DataFrame({
        "Tipo de Fraude": ["Fraude em Cart√£o de Cr√©dito", "Phishing", "Roubo de Identidade", "Transa√ß√µes Falsificadas"],
        "Medidas Preventivas": [
            "Ativar alertas de transa√ß√£o, usar autentica√ß√£o multifator e monitoramento cont√≠nuo.",
            "Nunca compartilhar dados pessoais, verificar remetentes suspeitos e utilizar autentica√ß√£o em dois fatores.",
            "Utilizar verifica√ß√£o biom√©trica, n√£o reutilizar senhas e ativar bloqueios autom√°ticos.",
            "Implementar monitoramento de transa√ß√µes em tempo real e an√°lises de comportamento."
        ]
    })
    st.table(fraud_prevention)

    # üí° Tecnologias e Estrat√©gias para Preven√ß√£o
    st.subheader("üí° Tecnologias e Estrat√©gias para Preven√ß√£o de Fraudes")
    st.write("""
    As empresas podem adotar as seguintes tecnologias para refor√ßar a seguran√ßa:
    - **Machine Learning & IA**: Modelos que analisam padr√µes e detectam anomalias.
    - **Autentica√ß√£o Multifator (MFA)**: Verifica√ß√£o em duas etapas para acessos financeiros.
    - **Monitoramento em Tempo Real**: Identifica√ß√£o de transa√ß√µes suspeitas √† medida que ocorrem.
    - **Criptografia Avan√ßada**: Prote√ß√£o de dados sens√≠veis contra acessos n√£o autorizados.
    - **An√°lises de Comportamento**: Identifica√ß√£o de padr√µes incomuns de uso do sistema.
    """)

# P√°gina 2: An√°lise de Fraudes
elif page == "üìä An√°lise de Fraudes":
    st.markdown('<p class="big-font">üìä An√°lise de Fraudes</p>', unsafe_allow_html=True)
    fraud = df[df["Class"] == 1]
    legit = df[df["Class"] == 0]

    # üî• Filtros Interativos
    st.subheader("üéØ Filtros de An√°lise")
    hora_selecionada = st.slider("Selecione um intervalo de hor√°rio", 0, 23, (0, 23))
    regiao_selecionada = st.multiselect("Filtrar por regi√£o", df["Region"].unique(), default=df["Region"].unique())

    fraude_filtrada = fraud[
        (fraud["Hour"].between(hora_selecionada[0], hora_selecionada[1])) &
        (fraud["Region"].isin(regiao_selecionada))
        ]

    # üìä Gr√°fico: Fraudes ao Longo do Dia
    st.subheader("üìÜ Distribui√ß√£o de Fraudes por Hor√°rio")
    fig, ax = plt.subplots(figsize=(8, 4))
    sns.histplot(fraude_filtrada["Hour"], bins=24, kde=True, color="red", ax=ax)
    ax.set_xlabel("Hora do Dia")
    ax.set_ylabel("N√∫mero de Fraudes")
    st.pyplot(fig)

    # üìç Fraudes por Regi√£o
    st.subheader("üåç Fraudes por Regi√£o")
    fraude_por_regiao = fraude_filtrada["Region"].value_counts(normalize=True) * 100
    fig, ax = plt.subplots(figsize=(8, 4))
    sns.barplot(x=fraude_por_regiao.index, y=fraude_por_regiao.values, palette="Reds_r", ax=ax)
    ax.set_ylabel("Percentagem de Fraudes (%)")
    st.pyplot(fig)

    # üìà Boxplot: Distribui√ß√£o dos Valores Fraudulentos
    st.subheader("üí∞ An√°lise dos Valores das Fraudes")
    fig, ax = plt.subplots(figsize=(8, 4))
    sns.boxplot(x=fraude_filtrada["Amount"], color="red", ax=ax)
    ax.set_xlabel("Valor da Fraude ($)")
    st.pyplot(fig)

    # üìä Heatmap: Fraudes por Hora e Regi√£o
    st.subheader("üî• Mapa de Calor: Fraudes por Hora e Regi√£o")
    heatmap_data = fraud.pivot_table(index="Region", columns="Hour", values="Class", aggfunc="count", fill_value=0)
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.heatmap(heatmap_data, cmap="Reds", linewidths=0.5, ax=ax)
    st.pyplot(fig)

    # üìå Insights Autom√°ticos
    st.subheader("üìå Insights Autom√°ticos")
    if len(fraude_filtrada) > 0:
        max_hora = fraude_filtrada["Hour"].value_counts().idxmax()
        max_regiao = fraude_filtrada["Region"].mode()[0]
        st.write(f"üìå **A maior concentra√ß√£o de fraudes ocorre √†s {max_hora}h.**")
        st.write(f"üìå **A regi√£o mais afetada √© {max_regiao}.**")
        st.write(f"üìå **O valor m√©dio das fraudes √© ${fraude_filtrada['Amount'].mean():.2f}.**")
        st.write(f"üìå **O maior valor de fraude registrado foi ${fraude_filtrada['Amount'].max():.2f}.**")
    else:
        st.write("‚úÖ Nenhuma fraude encontrada para os filtros selecionados.")

    # üì§ Exporta√ß√£o de Dados
    st.subheader("üì• Exportar Dados Filtrados")
    csv_filtros = fraude_filtrada.to_csv(index=False).encode('utf-8')
    st.download_button(label="üì• Baixar CSV", data=csv_filtros, file_name="fraudes_filtradas.csv", mime="text/csv")


# üìà P√°gina de Estat√≠sticas
elif page == "üìà Estat√≠sticas":
    st.markdown('<p class="big-font">üìà Estat√≠sticas Avan√ßadas</p>', unsafe_allow_html=True)

    st.subheader("üìä M√©dias e Medianas")
    col1, col2 = st.columns(2)
    col1.write("### M√©dia:")
    col1.write(df.mean(numeric_only=True))
    col2.write("### Mediana:")
    col2.write(df.median(numeric_only=True))

    st.subheader("üìä Vari√¢ncia e Desvio Padr√£o")
    col1, col2 = st.columns(2)
    col1.write("### Vari√¢ncia:")
    col1.write(df.var(numeric_only=True))
    col2.write("### Desvio Padr√£o:")
    col2.write(df.std(numeric_only=True))

    # üî• Matriz de Correla√ß√£o
    st.subheader("üî• Matriz de Correla√ß√£o")
    fig, ax = plt.subplots(figsize=(10, 8))
    df_numeric = df.select_dtypes(include=["number"])
    sns.heatmap(df_numeric.corr(), cmap="coolwarm", annot=False, ax=ax)
    st.pyplot(fig)

    # Explica√ß√£o sobre as correla√ß√µes
    st.write("""
    üìå **An√°lise das Correla√ß√µes:**
    - **Correla√ß√µes Positivas Fortes:**
    - Rolling_Mean_Amount e Std_Amount: Correla√ß√£o positiva forte
    - Num_Transacoes_1h e algumas vari√°veis V: Correla√ß√µes positivas moderadas

    - **Correla√ß√µes Negativas Importantes:**
    - Delta_Amount e Rolling_Mean_Amount: Correla√ß√£o negativa moderada
    - Time_Diff e algumas vari√°veis V: Correla√ß√µes negativas moderadas
    - Delta_Media_Valor e Amount: Diferen√ßas em rela√ß√£o √† m√©dia tendem a ser inversas ao valor total da transa√ß√£o
    """)

    # üìä Matriz de Covari√¢ncia
    st.subheader("üìä Matriz de Covari√¢ncia")
    st.write(df_numeric.cov())

    # üìå An√°lise de Fraudes por Valor e Regi√£o
    st.subheader("üí∞ Fraudes por Valor e Regi√£o")
    fig, ax = plt.subplots(figsize=(10, 6))
    sns.violinplot(data=df, x="Region", y="Amount", hue="Class", split=True, ax=ax)
    ax.set_xlabel("Regi√£o")
    ax.set_ylabel("Valor da Transa√ß√£o")
    st.pyplot(fig)

    # üìå Insights Autom√°ticos
    st.markdown("### üìå Insights Autom√°ticos")
    if df["Class"].sum() > 0:
        regiao_mais_fraudulenta = df[df["Class"] == 1]["Region"].mode()[0]
        valor_medio_fraude = df[df["Class"] == 1]["Amount"].mean()
        st.write(f"üìå **A regi√£o com mais fraudes √© {regiao_mais_fraudulenta}.**")
        st.write(f"üìå **O valor m√©dio de uma transa√ß√£o fraudulenta √© de R$ {valor_medio_fraude:.2f}.**")
    else:
        st.write("Nenhuma fraude detectada nos dados.")

# P√°gina 4: Relat√≥rios e Configura√ß√µes
elif page == "üìÇ Relat√≥rios e Configura√ß√µes":
    st.markdown('<p class="big-font">üìÇ Relat√≥rios e Configura√ß√µes</p>', unsafe_allow_html=True)

    # Definindo sub-p√°ginas
    sub_page = st.sidebar.radio("Subt√≥picos", ["üìë Gerar Relat√≥rio", "‚öô Configura√ß√µes Avan√ßadas", "üîÑ Normaliza√ß√£o e Padroniza√ß√£o"])

    # üìë Gera√ß√£o de Relat√≥rios Personalizados
    if sub_page == "üìë Gerar Relat√≥rio":
        st.subheader("üì• Exporta√ß√£o de Dados")

        # üéØ Filtros Avan√ßados para o Relat√≥rio
        colunas_disponiveis = list(df.columns)
        colunas_selecionadas = st.multiselect("Selecione as colunas para o relat√≥rio", colunas_disponiveis,
                                              default=colunas_disponiveis)

        tipo_transacao = st.radio("Filtrar transa√ß√µes:", ["Todas", "Apenas Fraudes", "Apenas Leg√≠timas"])

        if tipo_transacao == "Apenas Fraudes":
            df_export = df[df["Class"] == 1]
        elif tipo_transacao == "Apenas Leg√≠timas":
            df_export = df[df["Class"] == 0]
        else:
            df_export = df.copy()

        df_export = df_export[colunas_selecionadas]

        # üìä Visualizar os dados antes do download
        st.write("üîç **Pr√©-visualiza√ß√£o dos Dados:**")
        st.dataframe(df_export.head(10))

        # üìä Distribui√ß√£o de Categorias de Montante
        st.subheader("üìä Distribui√ß√£o de Categorias de Montante")
        fig, ax = plt.subplots(figsize=(8, 4))
        df["Amount_Category"].value_counts().plot(kind="bar", color="skyblue", ax=ax)
        ax.set_xlabel("Categoria de Montante")
        ax.set_ylabel("N√∫mero de Transa√ß√µes")
        st.pyplot(fig)

        # üåô Propor√ß√£o de Transa√ß√µes Noturnas
        st.subheader("üåô Propor√ß√£o de Transa√ß√µes Noturnas")
        transacao_noturna = df["Transacao_Noturna"].value_counts(normalize=True) * 100
        st.write(f"**Transa√ß√µes Noturnas:** {transacao_noturna[1]:.2f}%")
        st.write(f"**Transa√ß√µes Diurnas:** {transacao_noturna[0]:.2f}%")

        # üìà M√©dia M√≥vel do Montante
        st.subheader("üìà M√©dia M√≥vel do Montante")
        fig, ax = plt.subplots(figsize=(10, 5))
        df["Rolling_Mean_Amount"].plot(ax=ax, color="blue", label="M√©dia M√≥vel (5 Transa√ß√µes)")
        ax.set_xlabel("√çndice")
        ax.set_ylabel("Montante ($)")
        ax.legend()
        st.pyplot(fig)

        # ‚è±Ô∏è Diferen√ßa de Tempo entre Transa√ß√µes
        st.subheader("‚è±Ô∏è Diferen√ßa de Tempo entre Transa√ß√µes")
        fig, ax = plt.subplots(figsize=(8, 4))
        sns.histplot(df["Time_Diff"].dropna(), bins=30, kde=True, color="purple", ax=ax)
        ax.set_xlabel("Diferen√ßa de Tempo (segundos)")
        ax.set_ylabel("Frequ√™ncia")
        st.pyplot(fig)

        # üî• Mapa de Calor: N√∫mero de Transa√ß√µes por Hora e Regi√£o
        st.subheader("üî• Mapa de Calor: N√∫mero de Transa√ß√µes por Hora e Regi√£o")
        heatmap_data = df.pivot_table(index="Region", columns="Hour", values="Num_Transacoes_1h", aggfunc="mean",
                                      fill_value=0)
        fig, ax = plt.subplots(figsize=(10, 6))
        sns.heatmap(heatmap_data, cmap="Blues", linewidths=0.5, ax=ax)
        st.pyplot(fig)

        # üìÇ Op√ß√µes de Exporta√ß√£o
        formato = st.selectbox("Escolha o formato do relat√≥rio:", ["CSV", "Excel"])
        if formato == "CSV":
            file_data = df_export.to_csv(index=False).encode('utf-8')
            file_name = "relatorio_fraude.csv"
            mime_type = "text/csv"
        else:
            file_data = df_export.to_excel(index=False).encode('utf-8')
            file_name = "relatorio_fraude.xlsx"
            mime_type = "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"

        st.download_button(label=f"üì• Baixar {formato}", data=file_data, file_name=file_name, mime=mime_type)

    # ‚öô Configura√ß√µes Avan√ßadas
    elif sub_page == "‚öô Configura√ß√µes Avan√ßadas":
        st.subheader("‚öô Ajustes do Sistema")

        # üìå Configura√ß√£o de Alertas de Fraude
        limite_alerta = st.slider("Definir limite de alerta para transa√ß√µes suspeitas ($):", 10, 5000, 1000)
        metodo_analise = st.radio("Escolha o m√©todo de detec√ß√£o de fraudes:", ["Regra Fixa", "Machine Learning"])

        # üåç Configura√ß√£o de Regi√µes
        st.subheader("üåé Personalizar An√°lise por Regi√£o")
        selected_region = st.multiselect("Selecione as regi√µes a monitorar:", df["Region"].unique(),
                                         default=df["Region"].unique())

        # üéØ Aplicar configura√ß√µes (Simula√ß√£o)
        if st.button("Salvar Configura√ß√µes"):
            st.success("‚úÖ Configura√ß√µes salvas com sucesso!")
            st.write(f"- **Limite de Alerta:** ${limite_alerta}")
            st.write(f"- **M√©todo de Detec√ß√£o:** {metodo_analise}")
            st.write(f"- **Regi√µes Monitoradas:** {', '.join(selected_region)}")

    # üîÑ Normaliza√ß√£o e Padroniza√ß√£o
    elif sub_page == "üîÑ Normaliza√ß√£o e Padroniza√ß√£o":
        st.subheader("üîÑ Padroniza√ß√£o e Normaliza√ß√£o de Dados")
        
        st.write("""
        ## Padroniza√ß√£o (Standardization)

        A padroniza√ß√£o (Z-score normalization) √© uma t√©cnica de pr√©-processamento de dados que transforma os valores 
        para que tenham m√©dia 0 e desvio padr√£o 1.
        """)

        # F√≥rmula matem√°tica com LaTeX
        st.latex(r'Z = \frac{X - \mu}{\sigma}')
        
        st.write("""
        onde:
        - X = valor original
        - Œº = m√©dia da distribui√ß√£o 
        - œÉ = desvio padr√£o da distribui√ß√£o
        
        **Caracter√≠sticas:**
        - Resulta em dados com m√©dia 0
        - Resulta em dados com desvio padr√£o 1
        - √ötil quando os dados seguem distribui√ß√£o normal
        - Preserva outliers (valores extremos)
        
        **Vantagens:**
        - Facilita a compara√ß√£o entre diferentes atributos
        - Essencial para algoritmos sens√≠veis √† escala (como SVM, K-means, PCA)
        - Melhora a converg√™ncia em algoritmos de gradient descent
        """)
        
        # Demonstra√ß√£o de padroniza√ß√£o com os dados
        with st.expander("üîç Demonstra√ß√£o de Padroniza√ß√£o"):
            # Selecionar uma coluna para demonstra√ß√£o
            selected_column = st.selectbox("Selecione uma coluna para padroniza√ß√£o:", 
                                          df.select_dtypes(include=['number']).columns)
            
            # Calcular m√©dia e desvio padr√£o
            mean_value = df[selected_column].mean()
            std_value = df[selected_column].std()
            
            # Criar uma amostra de dados padronizados
            original_data = df[selected_column].head(10).values
            standardized_data = (original_data - mean_value) / std_value
            
            # Mostrar uma compara√ß√£o
            comparison_df = pd.DataFrame({
                "Original": original_data,
                "Padronizado": standardized_data
            })
            
            st.write("**Dados Originais vs. Padronizados:**")
            st.write(comparison_df)
            
            # Mostrar estat√≠sticas
            st.write(f"**M√©dia Original:** {mean_value:.4f}")
            st.write(f"**Desvio Padr√£o Original:** {std_value:.4f}")
            st.write(f"**M√©dia dos Dados Padronizados:** {standardized_data.mean():.4f}")
            st.write(f"**Desvio Padr√£o dos Dados Padronizados:** {standardized_data.std():.4f}")
            
            # Plotar compara√ß√£o
            fig, ax = plt.subplots(1, 2, figsize=(10, 4))
            ax[0].hist(original_data, bins=10, color='blue', alpha=0.7)
            ax[0].set_title("Dados Originais")
            ax[1].hist(standardized_data, bins=10, color='green', alpha=0.7)
            ax[1].set_title("Dados Padronizados")
            st.pyplot(fig)
        
        st.write("""
        ## Normaliza√ß√£o (Min-Max Scaling)

        A normaliza√ß√£o transforma os dados para um intervalo espec√≠fico, tipicamente [0,1] ou [-1,1].
        """)
        
        # F√≥rmula matem√°tica com LaTeX
        st.latex(r"X' = \frac{X - X_{min}}{X_{max} - X_{min}}")
        
        st.write("""
        onde:
        - X = valor original
        - Xmin = valor m√≠nimo do atributo
        - Xmax = valor m√°ximo do atributo
        
        **Caracter√≠sticas:**
        - Escala os dados para um intervalo fixo
        - Preserva a distribui√ß√£o original dos dados
        - √ötil quando a distribui√ß√£o n√£o √© gaussiana
        - Mant√©m rela√ß√µes entre valores originais
        
        **Vantagens:**
        - Facilita compara√ß√£o entre vari√°veis de unidades diferentes
        - √ötil para algoritmos que exigem valores limitados
        - Boa para t√©cnicas como redes neurais e algoritmos baseados em dist√¢ncia
        """)
        
        # Demonstra√ß√£o de normaliza√ß√£o com os dados
        with st.expander("üîç Demonstra√ß√£o de Normaliza√ß√£o"):
            # Selecionar uma coluna para demonstra√ß√£o
            selected_column = st.selectbox("Selecione uma coluna para normaliza√ß√£o:", 
                                          df.select_dtypes(include=['number']).columns,
                                          key="normalization_column")
            
            # Calcular min e max
            min_value = df[selected_column].min()
            max_value = df[selected_column].max()
            
            # Criar uma amostra de dados normalizados
            original_data = df[selected_column].head(10).values
            normalized_data = (original_data - min_value) / (max_value - min_value)
            
            # Mostrar uma compara√ß√£o
            comparison_df = pd.DataFrame({
                "Original": original_data,
                "Normalizado": normalized_data
            })
            
            st.write("**Dados Originais vs. Normalizados:**")
            st.write(comparison_df)
            
            # Mostrar estat√≠sticas
            st.write(f"**Valor M√≠nimo Original:** {min_value:.4f}")
            st.write(f"**Valor M√°ximo Original:** {max_value:.4f}")
            st.write(f"**Valor M√≠nimo Normalizado:** {normalized_data.min():.4f}")
            st.write(f"**Valor M√°ximo Normalizado:** {normalized_data.max():.4f}")
            
            # Plotar compara√ß√£o
            fig, ax = plt.subplots(1, 2, figsize=(10, 4))
            ax[0].hist(original_data, bins=10, color='blue', alpha=0.7)
            ax[0].set_title("Dados Originais")
            ax[1].hist(normalized_data, bins=10, color='red', alpha=0.7)
            ax[1].set_title("Dados Normalizados")
            st.pyplot(fig)
        
        st.write("""
        ## Quando Usar Cada T√©cnica
        
        **Use Padroniza√ß√£o quando:**
        - Os dados seguem distribui√ß√£o normal ou pr√≥xima dela
        - O algoritmo pressup√µe normalidade dos dados
        - H√° presen√ßa significativa de outliers que n√£o devem ser ocultados
        - Trabalhando com algoritmos como SVM, regress√£o linear, ou PCA
        
        **Use Normaliza√ß√£o quando:**
        - Precisa de um intervalo espec√≠fico e limitado
        - Trabalhando com redes neurais, especialmente com fun√ß√µes de ativa√ß√£o que esperam entradas em [0,1] ou [-1,1]
        - A distribui√ß√£o dos dados n√£o √© gaussiana
        - A escala absoluta √© importante para o algoritmo
        
        ## Import√¢ncia no Big Data
        
        - Permite comparabilidade entre diferentes fontes de dados
        - Reduz o impacto de diferentes magnitudes entre vari√°veis
        - Essencial para algoritmos de aprendizado de m√°quina que s√£o sens√≠veis √† escala
        - Melhora a qualidade dos resultados de clustering e classifica√ß√£o
        - Facilita a integra√ß√£o de dados heterog√™neos
        """)
        
        # Aplica√ß√£o pr√°tica
        st.subheader("üß™ Aplica√ß√£o Pr√°tica")
        
        st.write("""
        Exemplo pr√°tico de como a padroniza√ß√£o e normaliza√ß√£o podem afetar a detec√ß√£o de fraudes:
        
        Considere as vari√°veis 'Amount' e 'Time' que possuem escalas muito diferentes. Um algoritmo de detec√ß√£o de fraude 
        baseado em dist√¢ncia (como KNN) daria peso desproporcional √† vari√°vel com maior magnitude. Ao normalizar ou 
        padronizar, ambas as vari√°veis t√™m peso equivalente na decis√£o do algoritmo.
        """)
        
        # Compara√ß√£o visual final
        st.subheader("üìä Compara√ß√£o Visual")
        
        # Selecionar duas colunas para visualiza√ß√£o
        col1, col2 = st.columns(2)
        with col1:
            selected_column1 = st.selectbox("Selecione a primeira coluna:", 
                                           df.select_dtypes(include=['number']).columns,
                                           key="vis_column1")
        with col2:
            selected_column2 = st.selectbox("Selecione a segunda coluna:", 
                                           df.select_dtypes(include=['number']).columns,
                                           key="vis_column2")
        
        # Amostrar dados para evitar sobrecarga
        sample_size = min(1000, len(df))
        sample_df = df.sample(sample_size, random_state=42)
        
        # Dados originais
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.scatter(sample_df[selected_column1], sample_df[selected_column2], 
                   c=sample_df['Class'], cmap='coolwarm', alpha=0.6)
        ax.set_xlabel(selected_column1)
        ax.set_ylabel(selected_column2)
        ax.set_title("Dados Originais")
        # Adicionar legenda manual
        import matplotlib.patches as mpatches
        red_patch = mpatches.Patch(color='red', label='Fraude')
        blue_patch = mpatches.Patch(color='blue', label='Leg√≠tima')
        ax.legend(handles=[red_patch, blue_patch])
        
        st.pyplot(fig)
        
        # Dados padronizados
        from sklearn.preprocessing import StandardScaler, MinMaxScaler
        
        # Preparar os dados
        X = sample_df[[selected_column1, selected_column2]].values
        y = sample_df['Class'].values
        
        # Padronizar
        scaler = StandardScaler()
        X_std = scaler.fit_transform(X)
        
        # Normalizar
        min_max_scaler = MinMaxScaler()
        X_norm = min_max_scaler.fit_transform(X)
        
        # Plotar dados padronizados
        fig, ax = plt.subplots(1, 2, figsize=(12, 5))
        
        # Padronizado
        ax[0].scatter(X_std[:, 0], X_std[:, 1], c=y, cmap='coolwarm', alpha=0.6)
        ax[0].set_xlabel(f"{selected_column1} (padronizado)")
        ax[0].set_ylabel(f"{selected_column2} (padronizado)")
        ax[0].set_title("Dados Padronizados")
        
        # Normalizado
        ax[1].scatter(X_norm[:, 0], X_norm[:, 1], c=y, cmap='coolwarm', alpha=0.6)
        ax[1].set_xlabel(f"{selected_column1} (normalizado)")
        ax[1].set_ylabel(f"{selected_column2} (normalizado)")
        ax[1].set_title("Dados Normalizados")
        
        plt.tight_layout()
        st.pyplot(fig)
        

# Nova p√°gina: Dados
elif page == "üß≠ Dados":
    st.markdown('<p class="big-font">üß≠ Dados</p>', unsafe_allow_html=True)

    st.subheader("üìä Dashboard de Vari√°veis")

    # Exibir as vari√°veis e seus valores
    variaveis_valores = {
        "Total de Transa√ß√µes": len(df),
        "Transa√ß√µes Fraudulentas": df["Class"].sum(),
        "Taxa de Fraude (%)": (df["Class"].sum() / len(df)) * 100,
        "Valor M√©dio das Transa√ß√µes ($)": df["Amount"].mean(),
        "Valor M√°ximo das Transa√ß√µes ($)": df["Amount"].max(),
        "Valor M√≠nimo das Transa√ß√µes ($)": df["Amount"].min(),
        "Desvio Padr√£o do Valor das Transa√ß√µes ($)": df["Amount"].std()
    }

    for variavel, valor in variaveis_valores.items():
        st.metric(label=variavel, value=f"{valor:,.2f}" if isinstance(valor, float) else f"{valor:,}")

    # Adicionar scope das vari√°veis
    st.subheader("üìÑ Scope das Vari√°veis")

    variaveis_escopo = {
        "Time": "Tempo decorrido desde a primeira transa√ß√£o no dataset.",
        "Vx": "Vari√°veis anonimizadas resultantes de PCA (28 componentes principais).",
        "Amount": "Montante da transa√ß√£o.",
        "Class": "Classe da transa√ß√£o (0: Leg√≠tima, 1: Fraudulenta).",
        "Hour": "Hora do dia em que a transa√ß√£o ocorreu.",
        "Rolling_Mean_Amount": "M√©dia m√≥vel do valor da transa√ß√£o (janela de 5 transa√ß√µes).",
        "Std_Amount": "Desvio padr√£o do valor da transa√ß√£o (janela de 5 transa√ß√µes).",
        "Delta_Amount": "Diferen√ßa entre o valor atual e o valor anterior da transa√ß√£o.",
        "Amount_Category": "Categoria do valor da transa√ß√£o (ex.: Muito Baixo, Baixo, M√©dio, etc.).",
        "Time_Diff": "Diferen√ßa de tempo entre transa√ß√µes consecutivas.",
        "Transacao_Noturna": "Indica se a transa√ß√£o ocorreu durante a noite (1: Sim, 0: N√£o).",
        "Num_Transacoes_1h": "N√∫mero de transa√ß√µes realizadas na mesma hora.",
        "Freq_Valor_Transacao": "Frequ√™ncia de transa√ß√µes com o mesmo valor.",
        "Delta_Media_Valor": "Diferen√ßa entre o valor da transa√ß√£o e a m√©dia m√≥vel.",
        "Region": "Regi√£o geogr√°fica associada √† transa√ß√£o."
    }

    for variavel, descricao in variaveis_escopo.items():
        st.write(f"**{variavel}:** {descricao}")

    # Adicionar gr√°ficos de valores m√≠nimo e m√°ximo
    st.subheader("üìä Gr√°ficos de Valores M√≠nimo e M√°ximo")

    # Agregar dados de V1-V28 em Vx
    df["Vx"] = df[[f"V{i}" for i in range(1, 29)]].sum(axis=1)
    min_vals = df[["Vx", "Hour", "Time_Diff"]].min()
    max_vals = df[["Vx", "Hour", "Time_Diff"]].max()

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar(min_vals.index, min_vals.values, color="blue", label="Min")
    ax.bar(max_vals.index, max_vals.values, color="red", label="Max", alpha=0.7)
    ax.set_title("Valores M√≠nimo e M√°ximo de Vx, Hour e Time_Diff")
    ax.set_ylabel("Valores")
    ax.legend()
    plt.xticks(rotation=90)
    st.pyplot(fig)

    # Adicionar gr√°ficos de valores m√≠nimo e m√°ximo
    st.subheader("üìä Gr√°ficos de Valores M√≠nimo e M√°ximo")

    # Remover colunas indesejadas
    columns_to_exclude = [f"V{i}" for i in range(1, 29)] + ["Vx", "Hour", "Time_Diff", "Class", "Transacao_Noturna"]
    numeric_columns = df.select_dtypes(include=['number']).columns
    numeric_columns = [col for col in numeric_columns if col not in columns_to_exclude]

    # Calcular valores m√≠nimos e m√°ximos apenas para colunas num√©ricas filtradas
    min_vals = df[numeric_columns].min()
    max_vals = df[numeric_columns].max()

    # Garantir que os √≠ndices sejam strings
    min_vals.index = min_vals.index.astype(str)
    max_vals.index = max_vals.index.astype(str)

    # Garantir que os valores sejam num√©ricos
    min_vals = pd.to_numeric(min_vals, errors='coerce').fillna(0)
    max_vals = pd.to_numeric(max_vals, errors='coerce').fillna(0)

    # Criar o gr√°fico
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar(min_vals.index, min_vals.values, color="blue", label="Min")
    ax.bar(max_vals.index, max_vals.values, color="red", label="Max", alpha=0.7)
    ax.set_title("Valores M√≠nimo e M√°ximo das Vari√°veis Especificadas")
    ax.set_ylabel("Valores")
    ax.legend()
    plt.xticks(rotation=90)
    st.pyplot(fig)

    # Adicionar legenda explicativa
    st.markdown("""
    **Legenda:**
    - **Min**: O valor m√≠nimo registrado para a vari√°vel.
    - **Max**: O valor m√°ximo registrado para a vari√°vel.
    Estes valores ajudam a entender a amplitude e a varia√ß√£o dos dados para cada vari√°vel.
    """)

# Nova p√°gina: Machine Learning
elif page == "ü§ñ Machine Learning":
    # Adicionar tabs para diferentes modelos de ML
    model_tabs = st.tabs(["Introdu√ß√£o", "Classifica√ß√£o", "Ridge e Lasso Regression"])
    
    with model_tabs[0]:
        # Mover o conte√∫do existente sobre ML para esta tab
        st.markdown("## Introdu√ß√£o ao Machine Learning")
        
        # Conceitos b√°sicos
        st.subheader("üîç Conceitos B√°sicos")
        st.write("""
        **Machine Learning (ML)** √© um subcampo da Intelig√™ncia Artificial que permite aos computadores aprender 
        sem programa√ß√£o expl√≠cita. Ao contr√°rio da programa√ß√£o tradicional onde escrevemos regras espec√≠ficas, 
        no ML os algoritmos aprendem padr√µes diretamente a partir dos dados.
        
        A principal diferen√ßa √© que em ML:
        - Os dados ensinam o computador
        - O sistema melhora com a experi√™ncia
        - Identifica padr√µes estatisticamente significativos
        """)
        
        # Compara√ß√£o visual entre programa√ß√£o tradicional e ML
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üíª Programa√ß√£o Tradicional")
            st.markdown("""
            ```
            Dados + Regras ‚Üí Resultados
            ```
            """)
            st.write("As regras s√£o definidas pelo programador")
            
        with col2:
            st.markdown("### ü§ñ Machine Learning")
            st.markdown("""
            ```
            Dados + Resultados ‚Üí Regras
            ```
            """)
            st.write("As regras s√£o descobertas pelo algoritmo")
        
        # Tipos de aprendizado
        st.subheader("üìö Tipos de Aprendizado")
        
        tab1, tab2, tab3 = st.tabs(["Supervisionado", "N√£o Supervisionado", "Por Refor√ßo"])
        
        with tab1:
            st.markdown("### Aprendizado Supervisionado")
            st.write("""
            No aprendizado supervisionado, o algoritmo √© treinado em um conjunto de dados rotulado, 
            onde para cada exemplo temos uma entrada e a sa√≠da desejada.
            
            **Exemplos de aplica√ß√µes:**
            - Classifica√ß√£o de e-mails em spam ou n√£o-spam
            - Previs√£o de pre√ßos de im√≥veis
            - Diagn√≥stico m√©dico
            
            **Algoritmos populares:**
            - Regress√£o Linear/Log√≠stica
            - √Årvores de Decis√£o
            - Random Forests
            - Support Vector Machines (SVM)
            - Redes Neurais
            """)
            
            # Demonstra√ß√£o visual simples
            st.markdown("#### Exemplo: Classifica√ß√£o de Fraudes")
            
            fig, ax = plt.subplots(figsize=(6, 4))
            
            # Amostra pequena para demonstra√ß√£o
            sample = df.sample(100, random_state=42)
            ax.scatter(sample["Amount"], sample["V1"], c=sample["Class"], cmap="coolwarm", s=50)
            ax.set_xlabel("Valor da Transa√ß√£o")
            ax.set_ylabel("Componente V1")
            ax.set_title("Exemplo de Classifica√ß√£o: Transa√ß√µes Leg√≠timas vs Fraudulentas")
            
            # Adicionar legenda manual
            import matplotlib.patches as mpatches
            red_patch = mpatches.Patch(color='red', label='Fraude')
            blue_patch = mpatches.Patch(color='blue', label='Leg√≠tima')
            ax.legend(handles=[red_patch, blue_patch])
            
            st.pyplot(fig)
        
        with tab2:
            st.markdown("### Aprendizado N√£o Supervisionado")
            st.write("""
            No aprendizado n√£o supervisionado, o algoritmo trabalha com dados n√£o rotulados, 
            buscando encontrar estruturas ou padr√µes intr√≠nsecos nos dados.
            
            **Exemplos de aplica√ß√µes:**
            - Segmenta√ß√£o de clientes
            - Agrupamento de not√≠cias semelhantes
            - Detec√ß√£o de anomalias
            - Redu√ß√£o de dimensionalidade
            
            **Algoritmos populares:**
            - K-means
            - DBSCAN
            - Hierarchical Clustering
            - PCA (Principal Component Analysis)
            - t-SNE
            """)
            
            # Demonstra√ß√£o visual de clustering
            st.markdown("#### Exemplo: Clustering de Transa√ß√µes")
            
            from sklearn.cluster import KMeans
            
            # Amostra para demonstra√ß√£o
            sample = df.sample(200, random_state=42)
            X = sample[["Amount", "V1"]].values
            
            # Aplicar K-means
            kmeans = KMeans(n_clusters=3, random_state=42)
            sample_clusters = kmeans.fit_predict(X)
            
            # Visualizar
            fig, ax = plt.subplots(figsize=(6, 4))
            scatter = ax.scatter(X[:, 0], X[:, 1], c=sample_clusters, cmap="viridis", s=50)
            ax.set_xlabel("Valor da Transa√ß√£o")
            ax.set_ylabel("Componente V1")
            ax.set_title("Clustering de Transa√ß√µes (K-means, k=3)")
            
            # Adicionar centr√≥ides
            ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
                      marker='X', s=200, color='red', label='Centr√≥ides')
            ax.legend()
            
            st.pyplot(fig)
        
        with tab3:
            st.markdown("### Aprendizado por Refor√ßo")
            st.write("""
            No aprendizado por refor√ßo, o algoritmo aprende a tomar decis√µes interagindo com um ambiente,
            recebendo recompensas ou penaliza√ß√µes pelas a√ß√µes tomadas.
            
            **Exemplos de aplica√ß√µes:**
            - Jogos (AlphaGo, Atari)
            - Rob√≥tica
            - Sistemas de recomenda√ß√£o
            - Trading automatizado
            
            **Algoritmos populares:**
            - Q-Learning
            - Deep Q-Network (DQN)
            - Policy Gradient
            - Actor-Critic
            """)
            
            st.image("https://cdn-images-1.medium.com/max/800/1*Z2yMvuRTXcMHRdHzKMRM5w.png", 
                    caption="Ciclo de Aprendizado por Refor√ßo", width=400)
    
    # Processo de Machine Learning
    st.subheader("‚öôÔ∏è Processo de Machine Learning")
    
    process_steps = {
        "1. Prepara√ß√£o de Dados": "Coleta, limpeza, normaliza√ß√£o e divis√£o em conjuntos de treinamento/teste",
        "2. Sele√ß√£o de Modelo": "Escolha do algoritmo mais adequado para o problema",
        "3. Treinamento": "Ajuste dos par√¢metros do modelo usando dados de treinamento",
        "4. Valida√ß√£o": "Avalia√ß√£o do desempenho em dados n√£o vistos anteriormente",
        "5. Ajuste de Hiperpar√¢metros": "Otimiza√ß√£o do modelo para melhorar o desempenho",
        "6. Implanta√ß√£o": "Coloca√ß√£o do modelo em produ√ß√£o",
        "7. Monitoramento": "Acompanhamento cont√≠nuo do desempenho"
    }
    
    col1, col2 = st.columns(2)
    
    for i, (step, desc) in enumerate(process_steps.items()):
        if i < 4:
            col1.markdown(f"**{step}:** {desc}")
        else:
            col2.markdown(f"**{step}:** {desc}")
    
    # Aplica√ß√µes em detec√ß√£o de fraude
    st.subheader("üí≥ Machine Learning na Detec√ß√£o de Fraudes")
    
    st.write("""
    A detec√ß√£o de fraudes √© uma das aplica√ß√µes mais importantes de machine learning no setor financeiro.
    Algoritmos ML podem identificar padr√µes suspeitos e anomalias que seriam dif√≠ceis de detectar manualmente.
    
    **Benef√≠cios:**
    
    - **Processamento em tempo real**: an√°lise de transa√ß√µes √† medida que ocorrem
    - **Adaptabilidade**: aprendizado cont√≠nuo com novos padr√µes de fraude
    - **Redu√ß√£o de falsos positivos**: melhoria na precis√£o da detec√ß√£o
    - **Escalabilidade**: capacidade de processar milh√µes de transa√ß√µes
    
    **Desafios:**
    
    - **Dados desbalanceados**: geralmente h√° muito mais transa√ß√µes leg√≠timas que fraudulentas
    - **Adapta√ß√£o a novas fraudes**: fraudadores evoluem constantemente suas t√©cnicas
    - **Lat√™ncia**: necessidade de respostas em milissegundos
    - **Dados sens√≠veis**: quest√µes de privacidade e seguran√ßa
    """)
    
    # M√©tricas de avalia√ß√£o
    st.subheader("üìè M√©tricas de Avalia√ß√£o em Detec√ß√£o de Fraudes")
    
    metrics = {
        "Acur√°cia": "Porcentagem total de previs√µes corretas",
        "Precis√£o": "Entre os casos classificados como fraude, quantos realmente s√£o fraude",
        "Recall (Sensibilidade)": "Entre as fraudes reais, quantas foram detectadas corretamente",
        "F1-Score": "M√©dia harm√¥nica entre precis√£o e recall",
        "AUC-ROC": "Capacidade de distinguir entre classes (0.5 = aleat√≥rio, 1.0 = perfeito)",
        "Custo de classifica√ß√£o errada": "Perda financeira devido a falsos positivos e falsos negativos"
    }
    
    for metric, desc in metrics.items():
        st.markdown(f"**{metric}**: {desc}")
    
    # Demonstra√ß√£o pr√°tica
    st.subheader("üß™ Demonstra√ß√£o Pr√°tica")
    
    with st.expander("Clique para ver uma demonstra√ß√£o simplificada de detec√ß√£o de fraudes"):
        st.write("""
        Abaixo est√° um exemplo simplificado de como um modelo de classifica√ß√£o pode ser usado para detectar fraudes.
        
        Este exemplo usa apenas duas vari√°veis para facilitar a visualiza√ß√£o, mas modelos reais usariam m√∫ltiplas vari√°veis.
        """)
        
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
        
        # Preparar dados (amostra pequena para demonstra√ß√£o r√°pida)
        sample = df.sample(1000, random_state=42)
        X = sample[["Amount", "V1", "V3", "V4"]].values
        y = sample["Class"].values
        
        # Dividir em treino e teste
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
        
        # Treinar modelo
        with st.spinner('Treinando o modelo...'):
            model = RandomForestClassifier(n_estimators=10, random_state=42)
            model.fit(X_train, y_train)
        
        # Fazer previs√µes
        y_pred = model.predict(X_test)
        
        # Avaliar modelo
        st.write("**Acur√°cia do modelo:**", accuracy_score(y_test, y_pred))
        
        # Matriz de confus√£o
        cm = confusion_matrix(y_test, y_pred, labels=[0, 1])  # Especificamos explicitamente as classes 0 e 1
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=['Leg√≠tima', 'Fraude'], yticklabels=['Leg√≠tima', 'Fraude'])
        ax.set_xlabel('Previsto')
        ax.set_ylabel('Real')
        ax.set_title('Matriz de Confus√£o')
        st.pyplot(fig)
        
        # Relat√≥rio de classifica√ß√£o
        st.write("**Relat√≥rio de classifica√ß√£o:**")
        st.text(classification_report(y_test, y_pred, zero_division=0))
        
        # Import√¢ncia das features
        importances = model.feature_importances_
        feature_names = ["Amount", "V1", "V3", "V4"]
        
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.bar(feature_names, importances)
        ax.set_ylabel('Import√¢ncia')
        ax.set_title('Import√¢ncia das Features')
        st.pyplot(fig)
        
        st.write("""
        **Observa√ß√£o:** Este √© apenas um exemplo simplificado para fins educativos. 
        Em cen√°rios reais, seriam necess√°rios:
        - Pr√©-processamento mais extenso dos dados
        - Utiliza√ß√£o de mais features
        - Ajuste de hiperpar√¢metros
        - T√©cnicas para lidar com dados desbalanceados
        - Valida√ß√£o cruzada
        """)
    
    with model_tabs[1]:
        # Mover a demonstra√ß√£o de classifica√ß√£o para esta tab
        st.markdown("## Classifica√ß√£o para Detec√ß√£o de Fraudes")
        
        # Carregar dados
        df = pd.read_csv("creditcard.csv")
        df = df.dropna()
        
        # Criar vari√°vel alvo (Class) desbalanceada
        df["Class"] = df["Class"].astype("category")
        
        # Amostra dos dados
        st.subheader("Amostra dos Dados")
        st.write(df.sample(10))
        
        # Contagem das classes
        st.subheader("Distribui√ß√£o das Classes")
        class_counts = df["Class"].value_counts()
        st.bar_chart(class_counts)
        
        # Sele√ß√£o de vari√°veis
        st.subheader("Sele√ß√£o de Vari√°veis")
        
        all_columns = df.columns.tolist()
        target = "Class"
        features = st.multiselect(
            "Selecione as vari√°veis independentes (features):",
            options=all_columns,
            default=all_columns[:-1]  # Selecionar todas menos a √∫ltima (que √© a vari√°vel alvo)
        )
        
        # Garantir que a vari√°vel alvo n√£o esteja entre as features selecionadas
        if target in features:
            features.remove(target)
        
        st.write("Features selecionadas:", features)
        
        # Dividir dados
        X = df[features]
        y = df[target]
        
        # Dividir em treino e teste
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
        
        # Treinamento do modelo
        st.subheader("Treinamento do Modelo")
        
        # Selecionar modelo
        model_type = st.selectbox(
            "Escolha o tipo de modelo:",
            ["Random Forest", "Regress√£o Log√≠stica", "√Årvore de Decis√£o"]
        )
        
        if model_type == "Random Forest":
            from sklearn.ensemble import RandomForestClassifier
            model = RandomForestClassifier(class_weight='balanced', random_state=42)
        elif model_type == "Regress√£o Log√≠stica":
            from sklearn.linear_model import LogisticRegression
            model = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=2000, random_state=42)
        else:
            from sklearn.tree import DecisionTreeClassifier
            model = DecisionTreeClassifier(random_state=42)
        
        # Treinar modelo
        with st.spinner(f'Treinando o modelo ({model_type})...'):
            # Aplicar SMOTE para balancear as classes
            smote = SMOTE(random_state=42)
            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
            
            model.fit(X_train_resampled, y_train_resampled)
        
        # Avalia√ß√£o do modelo
        st.subheader("Avalia√ß√£o do Modelo")
        
        # Fazer previs√µes
        y_pred = model.predict(X_test)
        
        # Acur√°cia
        accuracy = accuracy_score(y_test, y_pred)
        st.write(f"Acur√°cia: {accuracy:.2f}")
        
        # Matriz de confus√£o
        st.subheader("Matriz de Confus√£o")
        cm = confusion_matrix(y_test, y_pred, labels=[0, 1])  # Especificamos explicitamente as classes 0 e 1
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=['Leg√≠tima', 'Fraude'], yticklabels=['Leg√≠tima', 'Fraude'])
        ax.set_xlabel('Previsto')
        ax.set_ylabel('Real')
        ax.set_title('Matriz de Confus√£o')
        st.pyplot(fig)
        
        # Relat√≥rio de classifica√ß√£o
        st.subheader("Relat√≥rio de Classifica√ß√£o")
        st.text(classification_report(y_test, y_pred, zero_division=0))
        
        # Import√¢ncia das features (apenas para Random Forest)
        if model_type == "Random Forest":
            st.subheader("Import√¢ncia das Features")
            
            importances = model.feature_importances_
            feature_names = features
            
            # Criar dataframe de import√¢ncias
            importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Import√¢ncia': importances
            }).sort_values(by="Import√¢ncia", ascending=False)
            
            st.write(importance_df)
            
            # Gr√°fico de barras
            fig, ax = plt.subplots(figsize=(8, 6))
            sns.barplot(data=importance_df, x="Import√¢ncia", y="Feature", ax=ax, hue="Import√¢ncia", palette="viridis", legend=False)
            ax.set_title("Import√¢ncia das Features - Random Forest")
            ax.set_xlabel("Features")
            ax.set_ylabel("Import√¢ncia")
            st.pyplot(fig)
    
    with model_tabs[2]:
        st.markdown("## Ridge e Lasso Regression para Detec√ß√£o de Fraudes")
        
        st.write("""
        ### Regress√£o Regularizada para Classifica√ß√£o de Fraudes
        
        Embora Ridge e Lasso sejam t√©cnicas de regress√£o, elas podem ser aplicadas para problemas de classifica√ß√£o 
        bin√°ria como detec√ß√£o de fraudes. Neste exemplo, usaremos essas t√©cnicas para prever a vari√°vel 'Class'
        (0: transa√ß√£o leg√≠tima, 1: transa√ß√£o fraudulenta).
        
        - **Ridge Regression**: Utiliza regulariza√ß√£o L2, que penaliza a soma dos quadrados dos coeficientes.
        - **Lasso Regression**: Utiliza regulariza√ß√£o L1, que penaliza a soma dos valores absolutos dos coeficientes e pode reduzir alguns coeficientes a zero.
        """)
        
        # Sele√ß√£o de vari√°veis
        st.subheader("Configura√ß√£o do Modelo")

        # A vari√°vel alvo agora √© fixa como "Class"
        target_column = "Class"
        st.write(f"**Vari√°vel alvo:** {target_column} (0: Leg√≠tima, 1: Fraudulenta)")
        
        n_features = st.slider("N√∫mero de features a utilizar", 2, 10, 5)
        
        # Sele√ß√£o autom√°tica de features mais correlacionadas com a vari√°vel Class
        numeric_df = df.select_dtypes(include=['number'])
        if target_column in numeric_df.columns:
            correlations = numeric_df.drop(columns=[target_column]).corrwith(df[target_column]).abs().sort_values(ascending=False)
        else:
            correlations = numeric_df.corrwith(df[target_column]).abs().sort_values(ascending=False)
        best_features = correlations[:n_features].index.tolist()
        
        st.write(f"Features selecionadas (baseadas em correla√ß√£o com {target_column}):")
        st.write(best_features)
        
        # Dividir dados
        X = df[best_features].values
        y = df[target_column].values
        
        # Normalizar dados
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Dividir em treino e teste
        test_size = st.slider("Propor√ß√£o para teste (%)", 10, 40, 20) / 100
        X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=42, stratify=y)
        
        # Configura√ß√£o dos modelos
        st.subheader("Par√¢metros de Regulariza√ß√£o")
        
        col1, col2 = st.columns(2)
        with col1:
            alpha_ridge = st.slider(
                "Alpha para Ridge (for√ßa da regulariza√ß√£o L2):", 
                0.01, 10.0, 1.0, 0.01
            )
    
        with col2:
            alpha_lasso = st.slider(
                "Alpha para Lasso (for√ßa da regulariza√ß√£o L1):", 
                0.001, 1.0, 0.01, 0.001
            )
    
        # Treinamento dos modelos
        with st.spinner("Treinando modelos..."):
            # Linear Regression (sem regulariza√ß√£o)
            lr = LinearRegression()
            lr.fit(X_train, y_train)
            
            # Ridge Regression
            ridge = Ridge(alpha=alpha_ridge)
            ridge.fit(X_train, y_train)
            
            # Lasso Regression
            lasso = Lasso(alpha=alpha_lasso, max_iter=10000)
            lasso.fit(X_train, y_train)
        
        # Avalia√ß√£o dos modelos
        models = {
            "Regress√£o Linear": lr,
            f"Ridge (Œ±={alpha_ridge})": ridge,
            f"Lasso (Œ±={alpha_lasso})": lasso
        }
        
        # Configurar um limiar para converter previs√µes cont√≠nuas em bin√°rias
        threshold = 0.5
        
        results = {}
        predictions = {}
        
        for name, model in models.items():
            # Previs√µes cont√≠nuas
            y_pred_proba = model.predict(X_test)
            # Converter para bin√°rias usando threshold
            y_pred_binary = (y_pred_proba > threshold).astype(int)
            predictions[name] = y_pred_binary
            
            # M√©tricas de classifica√ß√£o
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            
            accuracy = accuracy_score(y_test, y_pred_binary)
            precision = precision_score(y_test, y_pred_binary, zero_division=0)
            recall = recall_score(y_test, y_pred_binary, zero_division=0)
            f1 = f1_score(y_test, y_pred_binary, zero_division=0)
            mse = mean_squared_error(y_test, y_pred_proba)
            
            results[name] = {
                "Acur√°cia": accuracy,
                "Precis√£o": precision,
                "Recall": recall, 
                "F1-Score": f1,
                "MSE": mse
            }
        
        # Mostrar resultados
        st.subheader("Resultados dos Modelos")
        
        # Criar dataframe de resultados
        results_df = pd.DataFrame({
            model: metrics
            for model, metrics in results.items()
        }).T
        
        st.write(results_df)
        
        # Gr√°fico de barras para F1-Score (melhor m√©trica para dados desbalanceados)
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.bar(results_df.index, results_df["F1-Score"], color=["blue", "green", "orange"])
        ax.set_ylabel('F1-Score')
        ax.set_title('Compara√ß√£o de Modelos - F1-Score (maior √© melhor)')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        st.pyplot(fig)
        
        # Visualizar coeficientes
        st.subheader("Coeficientes dos Modelos")
        
        coef_df = pd.DataFrame({
            'Feature': best_features,
            'Linear Regression': lr.coef_,
            f'Ridge (Œ±={alpha_ridge})': ridge.coef_,
            f'Lasso (Œ±={alpha_lasso})': lasso.coef_
        })
        
        st.write(coef_df.set_index('Feature'))
        
        # Gr√°fico de coeficientes
        fig, ax = plt.subplots(figsize=(12, 8))
        bar_width = 0.25
        index = np.arange(len(best_features))
        
        # Plotar barras para cada modelo
        ax.bar(index - bar_width, lr.coef_, bar_width, label='Linear Regression', color='blue')
        ax.bar(index, ridge.coef_, bar_width, label=f'Ridge (Œ±={alpha_ridge})', color='green')
        ax.bar(index + bar_width, lasso.coef_, bar_width, label=f'Lasso (Œ±={alpha_lasso})', color='orange')
        
        # Adicionar linha zero para refer√™ncia
        ax.axhline(y=0, color='gray', linestyle='-', alpha=0.3)
        
        # Configurar labels e legendas
        ax.set_xlabel('Features')
        ax.set_ylabel('Coeficientes')
        ax.set_title('Import√¢ncia das Features para Detec√ß√£o de Fraudes')
        ax.set_xticks(index)
        ax.set_xticklabels(best_features, rotation=45, ha='right')
        ax.legend()
        
        plt.tight_layout()
        st.pyplot(fig)
        
        # Matriz de confus√£o para o melhor modelo
        st.subheader("Matriz de Confus√£o")
        
        # Encontrar o melhor modelo com base no F1-Score
        best_model_name = results_df["F1-Score"].idxmax()
        best_model_pred = predictions[best_model_name]
        
        cm = confusion_matrix(y_test, best_model_pred, labels=[0, 1])
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=['Leg√≠tima', 'Fraude'], yticklabels=['Leg√≠tima', 'Fraude'])
        ax.set_xlabel('Previsto')
        ax.set_ylabel('Real')
        ax.set_title(f'Matriz de Confus√£o - {best_model_name}')
        st.pyplot(fig)
        
        # Explica√ß√£o sobre Ridge e Lasso para classifica√ß√£o
        st.subheader("Interpreta√ß√£o")
        
        st.write("""
        ### Aplica√ß√£o de Ridge e Lasso para Detec√ß√£o de Fraudes:
        
        1. **Interpreta√ß√£o dos Coeficientes**:
           - Coeficientes positivos: Indicam que valores maiores dessa feature aumentam a probabilidade de fraude
           - Coeficientes negativos: Indicam que valores maiores dessa feature diminuem a probabilidade de fraude
           - Coeficientes pr√≥ximos a zero (especialmente em Lasso): Indicam features menos relevantes para a detec√ß√£o
    
        2. **Compara√ß√£o dos Modelos**:
           - **Regress√£o Linear**: Sem regulariza√ß√£o, pode ser mais suscet√≠vel a overfitting, especialmente com muitas vari√°veis
           - **Ridge**: Reduz todos os coeficientes de forma proporcional, mantendo todas as features
           - **Lasso**: Tende a realizar sele√ß√£o de features, eliminando algumas completamente (coeficientes = 0)
    
        3. **Por que usar regulariza√ß√£o para fraudes?**
           - Dados de fraude geralmente t√™m muitas vari√°veis potencialmente correlacionadas
           - A regulariza√ß√£o ajuda a evitar overfitting em dados de treinamento
           - Lasso pode identificar automaticamente as vari√°veis mais importantes para detec√ß√£o
        """)
    
        # Adicionar thresholding interativo
        st.subheader("Ajuste de Limiar (Threshold)")
        
        st.write("""
        Em problemas de classifica√ß√£o desbalanceados como detec√ß√£o de fraudes, 
        ajustar o limiar de decis√£o √© crucial para equilibrar falsos positivos e falsos negativos.
        """)
        
        # Escolher um modelo para ajustar o threshold
        model_for_threshold = st.selectbox(
            "Escolha um modelo para ajustar o limiar:",
            list(models.keys())
        )
        
        # Obter as previs√µes cont√≠nuas
        selected_model = models[model_for_threshold]
        y_scores = selected_model.predict(X_test)
        
        # Slider para threshold
        custom_threshold = st.slider(
            "Limiar de decis√£o",
            min_value=0.0,
            max_value=1.0,
            value=0.5,
            step=0.01
        )
        
        # Aplicar threshold
        y_pred_custom = (y_scores > custom_threshold).astype(int)
        
        # M√©tricas com threshold personalizado
        custom_accuracy = accuracy_score(y_test, y_pred_custom)
        custom_precision = precision_score(y_test, y_pred_custom, zero_division=0)
        custom_recall = recall_score(y_test, y_pred_custom, zero_division=0)
        custom_f1 = f1_score(y_test, y_pred_custom, zero_division=0)
        
        # Mostrar m√©tricas
        col1, col2 = st.columns(2)
        col1.metric("Acur√°cia", f"{custom_accuracy:.4f}")
        col1.metric("Precis√£o", f"{custom_precision:.4f}")
        col2.metric("Recall", f"{custom_recall:.4f}")
        col2.metric("F1-Score", f"{custom_f1:.4f}")
        
        # Matriz de confus√£o com threshold personalizado
        cm_custom = confusion_matrix(y_test, y_pred_custom, labels=[0, 1])
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm_custom, annot=True, fmt='d', cmap='Blues', ax=ax, xticklabels=['Leg√≠tima', 'Fraude'], yticklabels=['Leg√≠tima', 'Fraude'])
        ax.set_xlabel('Previsto')
        ax.set_ylabel('Real')
        ax.set_title(f'Matriz de Confus√£o com Limiar = {custom_threshold}')
        st.pyplot(fig)
